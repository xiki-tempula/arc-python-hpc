{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python for High Performance Computing\n",
    "# The <span style=\"font-family: Courier New, Courier, monospace;\">mpi4py</span> library\n",
    "<hr style=\"border: solid 4px green\">\n",
    "<br>\n",
    "<center> <img src=\"images/arc_logo.png\"; alt=\"Logo\" style=\"float: center; width: 20%\"></center>\n",
    "<br>\n",
    "## http://www.arc.ox.ac.uk\n",
    "## support@arc.ox.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### Tools\n",
    "* the `mpi4py` Python module\n",
    "* an underlying MPI library\n",
    "<br><br>\n",
    "\n",
    "### Install (the Anaconda distribution)\n",
    "* easy option (laptops, desktops)\n",
    "  * install `mpi4py` plus a MPI library (*e.g.* the MPICH implementation) using `conda`\n",
    "```bash\n",
    "local-shell> conda install --channel mpi4py mpich mpi4py\n",
    "```\n",
    "* advanced option (HPC environment):\n",
    "  * install a MPI library to exploit the fast inter-node network\n",
    "  * install `mpi4py` on top of that MPI library and using Anaconda Python\n",
    "\n",
    "> *Note*: avoid name clashes in `PATH`, see http://conda.pydata.org/docs/using/envs.html\n",
    "<br><br>\n",
    "\n",
    "### MPI and the iPython notebook\n",
    "* *bad*: use `mpi4py` directly in notebook cells\n",
    "* *better*: use the `ipyparallel` package\n",
    "* *easiest* (this presentation): use `mpi4py` in scripts and rely on shell escape to run parallel examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning MPI\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### This presentation assumes (at least some) familiarity with MPI\n",
    "<br><br>\n",
    "\n",
    "### Many options for learning MPI\n",
    "* traditional courses\n",
    "  * ARC course dedicated to MPI\n",
    "  * Archer training\n",
    "* plenty of online material\n",
    "  * *e.g.* http://www.archer.ac.uk/training/online/\n",
    "<br><br>\n",
    "\n",
    "### Advice: familiarise yourselves with MPI using C or Fortran\n",
    "* material is generally a lot more detailed than Python documentation\n",
    "* a lot more online tutorials and good textbooks than for Python\n",
    "* clearer learning path than for Python (MPI for Python has several interfaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is MPI?\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### One standard\n",
    "* **M**essage **P**assing **I**nterface\n",
    "* defines the syntax and semantics of a core of library routines designed for writing portable message-passing programs (in C and Fortran)\n",
    "* the *de facto* standard for distributed parallel programming\n",
    "<br><br>\n",
    "\n",
    "### Many implementations\n",
    "* open source, *e.g.* OpenMPI, Mvapich2, MPICH\n",
    "* commercial, *e.g.* IntelMPI\n",
    "<br><br>\n",
    "\n",
    "### Several interfaces\n",
    "* C, Fortran (the MPI primary target)\n",
    "* Java, Python (late additions)\n",
    "\n",
    "> *Note*: the C++ interface was removed from the version 3 standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Message passing paradigm\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### Distributed memory programming model\n",
    "* independent processes running concurrently\n",
    "* all processes execute the *same* program\n",
    "* each process has its own memory address space\n",
    "* processes usually need some data from each other\n",
    "  * the data is passed explicitly\n",
    "  * data (message) passing is achieved via MPI programming\n",
    "<br><br>\n",
    "\n",
    "### Distributed computing *and* distributed memory\n",
    "* the problem data is partitioned, each processes owns a partition and works on it\n",
    "* computation becomes *possible* because large *memory* requirements are *distributed*\n",
    "* computation becomes *faster* because large *computational* requirements are *distributed*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Message passing paradigm (cont'd)\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### Advantage #1: universality & portability\n",
    "* works everywhere and matches all hardware\n",
    "  * separate processors connected by any network (*e.g.* modern clusters, *ad hoc* networks of computers)\n",
    "  * shared memory systems (*e.g.* your laptop)\n",
    "* source code is universally portable (with few exceptions, *e.g.* parallel I/O)\n",
    "  * between computer systems\n",
    "  * between implementations\n",
    "<br><br>\n",
    "\n",
    "### Advantage #2: performance & scalability\n",
    "* the most compelling reason why MPI remains a permanent component of HPC\n",
    "* with future core count increase, distributed memory computing is the key to high performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MPI for Python\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### MPI and Python\n",
    "* the MPI standard says nothing about Python\n",
    "* any MPI solution for Python mimics the C/C++ standard\n",
    "<br><br>\n",
    "\n",
    "### Several options\n",
    "* old: `pypar`, `pyMPI`, Scientific Python\n",
    "* newer and better: `mpi4py`\n",
    "<br><br>\n",
    "\n",
    "### <span style=\"font-family: Courier New, Courier, monospace;\">mpi4py</span> combines the MPI functionality at Python programmability\n",
    "* an interface very similar to the MPI-2 standard C++ interface\n",
    "* focus is in translating MPI syntax and semantics (if you know MPI, `mpi4py` is \"obvious\")\n",
    "* can communicate memory-contiguous data (as C/Fortran) and can communicate Python objects\n",
    "* performance not the same as for C/Fortran but what is lost in performance is gained in code development time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MPI for Python (cont'd)\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### Observations\n",
    "* some appreciation of the object-oriented nature of Python programming is useful\n",
    "* the C++ bindings in the MPI-2 standard can provide a useful quick reference for `mpi4py`\n",
    "* the C++ function names can be slightly different from the corresponding C/Fortran bindings\n",
    "  * often, they are reversed, *e.g.* `MPI_Buffer_attach()` becomes `MPI.Attach_buffer()`\n",
    "* the iPython help `help(mpi4py.MPI)` is rather long\n",
    "  * useful to narrow it down, *e.g.*, `help(mpi4py.MPI.Intracomm)`\n",
    "  * this requires that you know what you are looking for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: \"hello world!\"\n",
    "<hr style=\"border: solid 4px green\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, run the example executing the cell\n",
    "# %load helloworld.py\n",
    "\"\"\"Hellow world!  Importing and using MPI.COMM_WORLD\"\"\"\n",
    "\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "\n",
    "def report (communicator):\n",
    "    \"\"\"Report rank and size of this communicator\"\"\"\n",
    "    rank = communicator.rank\n",
    "    size = communicator.size\n",
    "    sys.stdout.write (\"Hello from rank {:2d} of {:2d}\\n\".format(rank, size))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Execute in MPI.COMM_WORLD\"\"\"\n",
    "    report (MPI.COMM_WORLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from rank  0 of  4\r\n",
      "Hello from rank  1 of  4\r\n",
      "Hello from rank  2 of  4\r\n",
      "Hello from rank  3 of  4\r\n"
     ]
    }
   ],
   "source": [
    "# second, run the example using the MPI launcher\n",
    "! mpirun -np 4 python helloworld.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"font-family: Courier New, Courier, monospace;\">mpi4py</span> basics\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### The Python script is started through the MPI launcher <span style=\"font-family: Courier New, Courier, monospace;\">mpirun</span>\n",
    "* `mpirun` is part of the MPI library implementation (nothing to do with Python)\n",
    "* used to launch *any* MPI code, *e.g.* code compiled from C/C++/Fortran source\n",
    "* two principal command line arguments\n",
    "  * the number of processes to launch\n",
    "  * (optional) the hosts on which to run these processes (default: local host)\n",
    "<br><br>\n",
    "\n",
    "### The launcher starts and coordinates a number of processes\n",
    "* all these processes run concurrently\n",
    "* all processes execute the *same* Python code\n",
    "* programming differentiates between what processes do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"font-family: Courier New, Courier, monospace;\">mpi4py</span> basics (cont'd)\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### MPI initialisation\n",
    "* import the `mpi4py` module\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "```\n",
    "* the `import mpi4py` statement is responsible for initialising MPI (if not already initialised)\n",
    "  * no analogue to the C/Fortran calls to `MPI_Init()` and `MPI_Finalize()`\n",
    "  * the underlying MPI library ensures that\n",
    "    * `MPI_Init()` is implicit to the `mpi4py` module being imported\n",
    "    * `MPI_Finalize()` is implicit when Python process terminates\n",
    "<br><br>\n",
    "\n",
    "### The class <span style=\"font-family: Courier New, Courier, monospace;\">MPI</span>\n",
    "* loaded from module `mpi4py`\n",
    "* provides the pre-defined communicator `COMM_WORLD`\n",
    "* more about communicators in a moment...\n",
    "<br><br>\n",
    "\n",
    "### Size and rank\n",
    "* `size = MPI.COMM_WORLD.size` (or `MPI.COMM_WORLD.Get_size()`) is the number of the processes started through `mpirun`\n",
    "* `rank = MPI.COMM_WORLD.rank` (or `MPI.COMM_WORLD.Get_rank()`) is the ID of the process: $0\\leq \\text{rank}\\leq\\text{size}-1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Communicators\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### A communicator provides the context within which communication takes place\n",
    "* two or more processes can pass messages between each other *only* if they belong to the same communicator\n",
    "* all communicators are Python objects in `mpi4py`\n",
    "* the pre-defined communicator `COMM_WORLD` contains *all* processes started by `mpirun` and is accessed via the class `MPI`\n",
    "<br><br>\n",
    "\n",
    "### Class hierarchy for communicators\n",
    "```\n",
    "Comm\n",
    "    Intracomm\n",
    "    Intercomm\n",
    "        Topocomm\n",
    "            Cartcomm\n",
    "            Distgraphcomm\n",
    "            Graphcomm\n",
    "```\n",
    "* example: a Cartesian communicator `Cartcomm` can be derived from `COMM_WORLD`\n",
    "<br><br>\n",
    "\n",
    "### Many methods are implemented on `Comm` and inherited by subclasses\n",
    "* example\n",
    "```\n",
    "    comm.Barrier()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### Creating a Cartesian communicator\n",
    "`Cartcomm` object created from an existing `Intracomm` object uses, *e.g.*\n",
    "```\n",
    "Intracomm.Create_cart (self, dims, periods=None, reorder=False)\n",
    "```\n",
    "returns a new Cartesian communicator.\n",
    "<br><br>\n",
    "\n",
    "Methods for this object include `Get_cart_rank()`, `Get_coords()`, *etc.*, which are methods of the class `Cartcomm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load cartesian.py\n",
    "\"\"\"Creating a Cartesian Communicator\"\"\"\n",
    "\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "\n",
    "def createCart (parent):\n",
    "    \"\"\"Create a 2-d Cartesian communicator from parent communicator\"\"\"\n",
    "\n",
    "    dims = MPI.Compute_dims (parent.size, 2)\n",
    "\n",
    "    comm = parent.Create_cart (dims, periods = [True, False])\n",
    "    rank = comm.Get_rank ()\n",
    "    coords = comm.Get_coords (rank)\n",
    "    upx = comm.Shift (0, 1)\n",
    "    upy = comm.Shift (1, 1)\n",
    "\n",
    "    out = \"Rank{:2d} coords{:2d} {:2d} upx(src,dst) {} upy(src,dst) {}\\n\"\\\n",
    "                         .format(rank, coords[0], coords[1], upx, upy)\n",
    "    sys.stdout.write (out)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Execute in MPI.COMM_WORLD\"\"\"\n",
    "    createCart (MPI.COMM_WORLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 coords 0  0 upx(src,dst) (2, 2) upy(src,dst) (-2, 1)\r\n",
      "Rank 1 coords 0  1 upx(src,dst) (3, 3) upy(src,dst) (0, -2)\r\n",
      "Rank 2 coords 1  0 upx(src,dst) (0, 0) upy(src,dst) (-2, 3)\r\n",
      "Rank 3 coords 1  1 upx(src,dst) (1, 1) upy(src,dst) (2, -2)\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python ./cartesian.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MPI communication\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### Message passing\n",
    "* the transaction by which one process accesses data from another process\n",
    "* all communication has one (or several) sender(s) and one (or several) receiver(s)\n",
    "<br><br>\n",
    "\n",
    "### Patterns of communication\n",
    "* *point-to-point message passing* -- communication between only two processes\n",
    "* *collective message passing* -- communication between all processes (in a communicator)\n",
    "  * *one-to-many*\n",
    "  * *many-to-one*\n",
    "  * *all-to-all*\n",
    "<br><br>\n",
    "\n",
    "### Blocking or nonblocking?\n",
    "* *blocking messaging*: sender and receiver wait till the receive / send transaction finishes\n",
    "* *nonblocking messaging*: immediate send, no waiting, need to probe if a message arrived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"font-family: Courier New, Courier, monospace;\">mpi4py</span> communication\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### <span style=\"font-family: Courier New, Courier, monospace;\">mpi4py</span> supports\n",
    "* *fast* (near C-speed) communication of contiguous memory objects (typically, `NumPy` arrays)\n",
    "* *convenient* communication of generic Python objects (pickling behind the scenes)\n",
    "<br><br>\n",
    "\n",
    "### Communication of contiguous memory objects\n",
    "* function names start with upper-case: `Send()`, `Recv()`, `Bcast()`, `Scatter()`, etc.\n",
    "* the data communicated is *passed* as an argument to the function\n",
    "* arguments are explicitly specified, *e.g.* `[data, MPI.DOUBLE]` or `[data, count, MPI.DOUBLE]`\n",
    "* automatic MPI datatype discovery for NumPy arrays is supported, but limited to basic C types (all C/C99-native signed/unsigned integer types and single/double precision real/complex floating types)\n",
    "<br><br>\n",
    "\n",
    "\n",
    "### Communication of generic Python objects\n",
    "* function names start with lower-case: `send()`, `recv()`, `bcast()`, `scatter()`, etc.\n",
    "* any object to be communicated is *passed* as a paramenter to the function\n",
    "* the *received* object is the return value of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point-to-point communication: blocking <span style=\"font-family: Courier New, Courier, monospace;\">Send()</span> and <span style=\"font-family: Courier New, Courier, monospace;\">Recv()</span>\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### <span style=\"font-family: Courier New, Courier, monospace;\">Send()</span> and <span style=\"font-family: Courier New, Courier, monospace;\">Recv()</span>\n",
    "* communicate buffer-like (contiguous memory) objects\n",
    "* `Comm.Send (self, buf, int dest, int tag=0)`\n",
    "  * returns only when the data in the send buffer can be safely changed\n",
    "  * that does not mean the data arrived at destination\n",
    "* `Comm.Recv (self, buf, int source=ANY_SOURCE, int tag=ANY_TAG, Status status=None)`\n",
    "  * returns only when receive buffer contains data expected\n",
    "<br><br>\n",
    "\n",
    "### Observations\n",
    "* the interface makes significant use of optional arguments\n",
    "* unlike in the C/Fortran standard, count or data type arguments associated with the message buffer are optional as they can be inferred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point-to-point communication: blocking <span style=\"font-family: Courier New, Courier, monospace;\">Send()</span> and <span style=\"font-family: Courier New, Courier, monospace;\">Recv()</span> (cont'd)\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### The need to communicate contiguous arrays of data occurs frequently\n",
    "```python\n",
    "import numpy\n",
    "sz = 1000\n",
    "buf = numpy.zeros(sz, type = numpy.double)\n",
    "\n",
    "if rank == 0:\n",
    "    MPI.COMM_WORLD.Send ([buf, sz, MPI.DOUBLE], 1, tag = 99)\n",
    "elif rank == 1:\n",
    "    MPI.COMM_WORLD.Recv ([buf, sz, MPI.DOUBLE], source = 0, tag = 99)\n",
    "```\n",
    "\n",
    "### Count and data type are explicitly specified as part of a list but can be omitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point-to-point communication: blocking <span style=\"font-family: Courier New, Courier, monospace;\">Send()</span> and <span style=\"font-family: Courier New, Courier, monospace;\">Recv()</span> example\n",
    "<hr style=\"border: solid 4px green\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# %load send_recv_blocking.py\n",
    "\"\"\"A simple blocking Send/Recv pair\"\"\"\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "def main (comm):\n",
    "    \"\"\"Send a message between ranks 0 and 1\"\"\"\n",
    "\n",
    "    # buffer length\n",
    "    blen = 4\n",
    "    # process rank\n",
    "    rank = comm.Get_rank()\n",
    "\n",
    "    if rank == 0:\n",
    "        # create send buffer\n",
    "        buf = numpy.ones (blen, numpy.double)\n",
    "        # send buffer\n",
    "        comm.Send ( [buf, blen, MPI.DOUBLE], dest = 1, tag = 999 )\n",
    "    elif rank == 1:\n",
    "        # allocate space for recv buffer\n",
    "        buf = numpy.empty (blen, numpy.double)\n",
    "        # receive buffer\n",
    "        comm.Recv ( [buf, blen, MPI.DOUBLE], source = 0, tag = 999 )\n",
    "        print \" rank 1 received: %s\" % (buf)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main (MPI.COMM_WORLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rank 1 received: [ 1.  1.  1.  1.]\r\n"
     ]
    }
   ],
   "source": [
    "! mpirun -np 2 python ./send_recv_blocking.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point-to-point communication: <span style=\"font-family: Courier New, Courier, monospace;\">Send()</span> and <span style=\"font-family: Courier New, Courier, monospace;\">Recv()</span> pairs can lead to deadlocks\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### Deadlocks occur when the message passing cannot be completed\n",
    "* consider the classical example\n",
    "```python\n",
    "if rank == 0:\n",
    "    comm.Send ( [buf, blen, MPI.DOUBLE], dest = 1,   tag = 999 )\n",
    "    comm.Recv ( [buf, blen, MPI.DOUBLE], source = 1, tag = 999 )\n",
    "elif rank == 1:\n",
    "    comm.Send ( [buf, blen, MPI.DOUBLE], dest = 0,   tag = 999 )\n",
    "    comm.Recv ( [buf, blen, MPI.DOUBLE], source = 0, tag = 999 )\n",
    "```\n",
    "* `comm.Send` does not complete until the corresponding `comm.Recv` is posted and *vice versa*\n",
    "* `comm.Send` will never complete and the program will deadlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point-to-point communication: <span style=\"font-family: Courier New, Courier, monospace;\">Send()</span> and <span style=\"font-family: Courier New, Courier, monospace;\">Recv()</span> pairs can lead to deadlocks (cont'd)\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### Solutions\n",
    "* reverse the order of one of the send/receive pairs\n",
    "* *equivalent* order is send/receive for even ranks and reversed for odd\n",
    "* use `Sendrecv`, which performs a blocking send/receive transaction in one go\n",
    "* use non-blocking send/receive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point-to-point communication: non-blocking <span style=\"font-family: Courier New, Courier, monospace;\">Isend()</span> and <span style=\"font-family: Courier New, Courier, monospace;\">Irecv()</span>\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### Non-blocking Send and Receive: <span style=\"font-family: Courier New, Courier, monospace;\">Isend()</span> and <span style=\"font-family: Courier New, Courier, monospace;\">Irecv()</span>\n",
    "* communicate buffer-like (contiguous memory) objects\n",
    "* `Comm.Isend(self, buf, int dest, int tag=0)`\n",
    "* `Comm.Irecv(self, buf, int source=ANY_SOURCE, int tag=ANY_TAG)`\n",
    "* non-blocking (**I** stands for **i**mmediate send/receive)\n",
    "  * functions return at once, without waiting for transaction to complete\n",
    "  * functions return an object class `Request`\n",
    "<br><br>\n",
    "\n",
    "### <span style=\"font-family: Courier New, Courier, monospace;\">Request</span> objects\n",
    "* used to ensure the communication took place\n",
    "* handled by\n",
    "  * instance methods\n",
    "```python\n",
    "request.Wait (self, Status status=None)\n",
    "```\n",
    "  * or class methods\n",
    "```python\n",
    "Request.Waitall (type cls, requests, statuses=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point-to-point communication: non-blocking <span style=\"font-family: Courier New, Courier, monospace;\">Isend()</span> and <span style=\"font-family: Courier New, Courier, monospace;\">Irecv()</span> (cont'd)\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### What is the point?\n",
    "<br><br>\n",
    "\n",
    "### Hiding latencies by overlapping communication with processing\n",
    "* start non-blocking communication using `Isend()` and `Irecv()`\n",
    "* carry out some processing (which does *not* depend on data communicated)\n",
    "* handle the requests generated by `Isend()` and `Irecv()` to ensure communication takes place\n",
    "* carry on with processing that *does* depend on data communicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point-to-point communication: non-blocking <span style=\"font-family: Courier New, Courier, monospace;\">Isend()</span> and <span style=\"font-family: Courier New, Courier, monospace;\">Irecv()</span> example\n",
    "<hr style=\"border: solid 4px green\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load isend_irecv_nonblocking.py\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "def main (comm):\n",
    "    \"\"\"Exchange messages with 'ajoining' ranks\"\"\"\n",
    "    # right-hand adjoining rank (wraparound)\n",
    "    p1 = comm.rank + 1\n",
    "    if p1 >= comm.size: p1 = 0\n",
    "    # left-hand adjoining rank (wraparound)\n",
    "    m1 = comm.rank - 1\n",
    "    if m1 < 0: m1 = comm.size - 1\n",
    "\n",
    "    # send and recv buffers\n",
    "    smsg = numpy.array ( [comm.rank], numpy.int )\n",
    "    rmsg = numpy.zeros ( 2, numpy.int )\n",
    "\n",
    "    # initiate communication\n",
    "    reqs1 = comm.Isend (smsg, p1)\n",
    "    reqs2 = comm.Isend (smsg, m1)\n",
    "    reqr1 = comm.Irecv (rmsg[0:], source = p1)\n",
    "    reqr2 = comm.Irecv (rmsg[1:], source = m1)\n",
    "\n",
    "    # receive requests handled by Wait()\n",
    "    reqr1.Wait()\n",
    "    reqr2.Wait()\n",
    "\n",
    "    # all processes print\n",
    "    # NB: safe to print as messages were received already\n",
    "    print \"[%d] %s %s %s\" % (comm.rank, smsg, rmsg[1], rmsg[0])\n",
    "\n",
    "    # send requests handled by Waitall()\n",
    "    MPI.Request.Waitall ( [reqs1, reqs2] )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main(MPI.COMM_WORLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] [2] 1 3\r\n",
      "[0] [0] 3 1\r\n",
      "[3] [3] 2 0\r\n",
      "[1] [1] 0 2\r\n"
     ]
    }
   ],
   "source": [
    "! mpirun -np 4 python isend_irecv_nonblocking.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point-to-point communication: Python objects\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### Message passing of serialised (pickled) Python objects\n",
    "```python\n",
    "Comm.send (self, obj, int dest, int tag=0)\n",
    "Comm.recv (self, buf=None, int source=ANY_SOURCE, int tag=ANY_TAG, Status status=None)\n",
    "```\n",
    "<br><br>\n",
    "\n",
    "### Observations\n",
    "* serialisation / deserialisation carry an overhead (memory & time)\n",
    "* complex / large objects are slow to communicate\n",
    "<br><br>\n",
    "\n",
    "### Note\n",
    "* the incoming message is received as the return value\n",
    "```python\n",
    "msg = comm.recv(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point-to-point communication: Python objects example\n",
    "<hr style=\"border: solid 4px green\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load send_recv_list.py\n",
    "from mpi4py import MPI\n",
    "import sys\n",
    "\n",
    "def main (comm):\n",
    "    \"\"\"Send a list from rank 0 to rank 1\"\"\"\n",
    "\n",
    "    if comm.size != 2:\n",
    "        sys.stdout.write (\"Only two processes allowed\\n\")\n",
    "        comm.Abort(1)\n",
    "\n",
    "    if comm.rank == 0:\n",
    "        msg = [\"Any\", \"old\", \"thing\", comm.rank, {\"size\" : comm.size}]\n",
    "        comm.send (msg, dest=1, tag = 999)\n",
    "    elif comm.rank == 1:\n",
    "        msg = comm.recv (source=0, tag = 999)\n",
    "        print \" rank 1 received: %s\" % (msg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(MPI.COMM_WORLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rank 1 received: ['Any', 'old', 'thing', 0, {'size': 2}]\r\n"
     ]
    }
   ],
   "source": [
    "! mpirun -np 2 python send_recv_list.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collective communication\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### Multiple processes within same communicator exchange messages\n",
    "* *always* blocking\n",
    "* *all* processes in the communicator must call the collective\n",
    "* failing that, deadlocks occur\n",
    "<br><br>\n",
    "\n",
    "### Most useful\n",
    "* Broadcasts\n",
    "* Scatter / Gather\n",
    "* Reductions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collective communication (cont'd)\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### What is the point of collectives?\n",
    "* specialised functionality than can be replicated using point-to-point functions\n",
    "* example: a broadcast from rank 0 can be\n",
    "```python\n",
    "if comm.rank == 0:\n",
    "    for i in range (1, comm.size):\n",
    "        comm.Send ([buf, sz, MPI.DOUBLE], 1, tag = 99)\n",
    "else:\n",
    "    comm.Recv ([buf, sz, MPI.DOUBLE], source = 0, tag = 99)\n",
    "```\n",
    "<br><br>\n",
    "\n",
    "### Collectives are implemented as a tree-based communication\n",
    "* efficient -- $\\log N$ instead of $N$\n",
    "* accurate -- tree reductions are more accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collective communication: broadcast\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### One process sends the same message to all other processes in the communicator\n",
    "<img src=\"./images/bcast.png\"; alt=\"Logo\" style=\"float: center; width: 40%\">\n",
    "* continuous buffer (*e.g.* `numpy.ndarray`)\n",
    "```python\n",
    "comm.Bcast (self, buf, int root=0)\n",
    "```\n",
    "* Python objects (pickling behind the scenes)\n",
    "```python\n",
    "comm.bcast (self, buf, int root=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collective communication: broadcast example\n",
    "<hr style=\"border: solid 4px green; \">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load bcast.py\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "def main (comm):\n",
    "    if comm.rank == 0:\n",
    "        # rank 0 has ndarray data\n",
    "        u = numpy.arange (6, dtype=numpy.float64)\n",
    "        # and dict data\n",
    "        d = {\"x\": 1, \"y\": 3.14, \"z\": 1-2j}\n",
    "    else:\n",
    "        # all other have an empty array\n",
    "        u = numpy.empty (6, dtype=numpy.float64)\n",
    "        # and a place-holder\n",
    "        d = None\n",
    "\n",
    "    # broadcast from rank 0 to everybody\n",
    "    comm.Bcast ( [u, MPI.DOUBLE] , root=0 )\n",
    "    d = comm.bcast ( d, root=0 )\n",
    "\n",
    "    # all processes print data\n",
    "    print \"[%d] %s %s\" % (comm.rank, u, d[\"y\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main (MPI.COMM_WORLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mpirun -np 4 python bcast.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collective communication: scatter\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### One process sends a different message to each other processes in the communicator\n",
    "<img src=\"./images/scatter.png\"; alt=\"Logo\" style=\"float: center; width: 40%\">\n",
    "* continuous buffer (*e.g.* `numpy.ndarray`)\n",
    "```python\n",
    "comm.Scatter (self, sendbuf, recvbuf, int root=0)\n",
    "```\n",
    "* Python objects (pickling behind the scenes)\n",
    "```python\n",
    "comm.scatter (self, buf, int root=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collective communication: scatter example\n",
    "<hr style=\"border: solid 4px green; \">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load scatter.py\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "def main (comm):\n",
    "    if comm.rank == 0:\n",
    "        # rank 0 has ndarray data\n",
    "        sendBuf = numpy.empty ([comm.size, 8], dtype=int)\n",
    "        sendBuf.T[:,:] = range(comm.size)\n",
    "        # and list data\n",
    "        obj = [(i+1)**2 for i in range(comm.size)]\n",
    "    else:\n",
    "        # all other ranks have an empty send buffer\n",
    "        sendBuf = None\n",
    "        # and a place-holder\n",
    "        obj = None\n",
    "\n",
    "    # all ranks have a receiving buffer \n",
    "    recvBuf = numpy.empty (8, dtype=int)\n",
    "\n",
    "    # broadcast from rank 0 to everybody\n",
    "    comm.Scatter (sendBuf, recvBuf, root=0)\n",
    "    obj = comm.scatter ( obj, root=0 )\n",
    "\n",
    "    # check: all processes verify data\n",
    "    print \"[%d] %s %s\" % (comm.rank, recvBuf, obj)\n",
    "    # check: cleverer way is to use assert\n",
    "#   assert numpy.allclose (recvBuf, comm.rank)\n",
    "#   assert obj == (comm.rank+1)**2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main (MPI.COMM_WORLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mpirun -np 4 python scatter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collective communication: gather\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### One process receives a different message from each other processes in the communicator\n",
    "<img src=\"./images/gather.png\"; alt=\"Logo\" style=\"float: center; width: 40%\">\n",
    "* continuous buffer (*e.g.* `numpy.ndarray`)\n",
    "```python\n",
    "comm.Gather (self, sendbuf, recvbuf, int root=0)\n",
    "```\n",
    "* Python objects (pickling behind the scenes)\n",
    "```python\n",
    "comm.gather (self, buf, int root=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collective communication: gather example\n",
    "<hr style=\"border: solid 4px green; \">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load gather.py\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "def main (comm):\n",
    "    # ndarray data to send\n",
    "    sendBuf = numpy.zeros(8, dtype=int) + comm.rank\n",
    "    # ndarray data to receive\n",
    "    if comm.rank == 0:\n",
    "        recvBuf = numpy.empty ([comm.size, 8], dtype=int)\n",
    "    else:\n",
    "        recvBuf = None\n",
    "    # Python object\n",
    "    obj = (comm.rank+1)**2\n",
    "\n",
    "    # gather ndarray\n",
    "    comm.Gather (sendBuf, recvBuf, root=0)\n",
    "    # gather Python objects\n",
    "    obj = comm.gather(obj, root=0)\n",
    "\n",
    "    # check: all ranks print verify data\n",
    "    print \"[%d] %s %s\" % (comm.rank, recvBuf, obj)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main (MPI.COMM_WORLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mpirun -np 4 python gather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collective communication: reduction\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### Data in all processes is \"reduced\" to a single process according to an operation\n",
    "* operations are defined by the `Op` class in `mpi4py`\n",
    "  * pre-defined operations: `MIN`, `MAX`, `SUM`, etc.\n",
    "  * user defined operations can be programmed for\n",
    "<br><br>\n",
    "\n",
    "### Example\n",
    "* find the maximum value on all the data across all processes\n",
    "  * each process finds a \"local\" maximum across its own data\n",
    "  * all local maxima are \"reduced\" to a single value (the maximum across all data)\n",
    "<br><br>\n",
    "\n",
    "### Reductions on <span style=\"font-family: Courier New, Courier, monospace;\">NumPy</span> arrays\n",
    "* (memory-contiguous) buffers\n",
    "```python\n",
    "Comm.Reduce (self, sendbuf, recvbuf, Op op=SUM, int root=0)\n",
    "```\n",
    "* Python objects\n",
    "```python\n",
    "Comm.reduce (self, sendobj, Op=SUM, int root=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collective communication: reduction example\n",
    "<hr style=\"border: solid 4px green; \">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load reduce.py\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "def main(comm):\n",
    "\n",
    "    # ndarray data\n",
    "    buf  = ( numpy.ones (6, dtype=int) + 1 ) * comm.rank\n",
    "    buf2 = numpy.empty (6, dtype=int)\n",
    "\n",
    "    # Python object data\n",
    "    obj  = [ comm.rank ]\n",
    "\n",
    "    # reduction on ndarray: buf reduced to buff2\n",
    "    # comm.Reduce ( buf, buf2, op=MPI.SUM, root=0 ) # <- this works too\n",
    "    comm.Reduce ( [buf, MPI.INT], [buf2, MPI.INT], op=MPI.SUM, root=0 )\n",
    "\n",
    "    # reduction of Python object\n",
    "    obj  = comm.reduce ( [comm.rank], op=MPI.SUM, root=0 )\n",
    "\n",
    "    # all processes print data\n",
    "    if comm.rank == 0:\n",
    "        print \"[%d] %s %s <- reduced\" % (comm.rank, buf2, obj)\n",
    "    else:\n",
    "        print \"[%d] %s %s\" % (comm.rank, buf, obj)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(MPI.COMM_WORLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] [6 6 6 6 6 6] None\r\n",
      "[1] [2 2 2 2 2 2] None\r\n",
      "[2] [4 4 4 4 4 4] None\r\n",
      "[0] [12 12 12 12 12 12] [0, 1, 2, 3] <- reduced\r\n"
     ]
    }
   ],
   "source": [
    "! mpirun -np 4 python reduce.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other features\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### Standard MPI functionality is available (<span style=\"font-family: Courier New, Courier, monospace;\">mpi4py</span> version 2.0)\n",
    "* `comm.Abort()` (terminate MPI execution environment) , `MPI.Wtime()` (accurate elapsed time), ...\n",
    "* user-defined types (methods implemented in the `Datatype` class)\n",
    "* RMA (remote single-sided) communication (see the `Win` class)\n",
    "* MPI-IO (see the `File` class)\n",
    "* a few functions are not implemented, *e.g.* `AlltoAllw()` (all processes send data of different types to, and receive data of different types from, all processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some other considerations\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### Performance notes\n",
    "* problem: `import` statements trigger a lot of small-file I/O\n",
    "  * in parallel calculations, all proceses perform same I/O\n",
    "  * large numbers of MPI tasks can cause severe logjam at `import`\n",
    "  * a serious obstacle to Python scaling\n",
    "* solution #1: install modules read by `import` in ramdisk\n",
    "* solution #2: start in Python and call another language\n",
    "  * can typically pass a communicator and other relevant data\n",
    "  * convenient access to C `MPI_Comm` handles requires `mpi4py` 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### We have briefly looked at\n",
    "* the message passing paradigm and the MPI standard\n",
    "* the `mpi4py` module in Python\n",
    "<br><br>\n",
    "\n",
    "###  <span style=\"font-family: Courier New, Courier, monospace;\">mpi4py</span> for production code?\n",
    "* yes, if\n",
    "  * communication is not very frequent\n",
    "  * communication does not involve a lot of data\n",
    "  * performance is not the primary concern\n",
    "* no, if\n",
    "  * the algorithm requires a lot of communication\n",
    "  * the plan is to scale to large core counts\n",
    "\n",
    "### A good idea (?)\n",
    "* `mpi4py` may be a good tool for teaching the basic concepts of distributed (MPI) computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<img src=\"../../images/reusematerial.png\"; style=\"float: center; width: 90\"; >"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
