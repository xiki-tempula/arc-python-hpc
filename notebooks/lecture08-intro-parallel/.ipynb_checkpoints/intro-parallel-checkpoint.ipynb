{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python for High Performance Computing\n",
    "# Parallel Programming\n",
    "<hr style=\"border: solid 4px green\">\n",
    "<br>\n",
    "<center> <img src=\"images/arc_logo.png\"; alt=\"Logo\" style=\"float: center; width: 20%\"></center>\n",
    "<br>\n",
    "## http://www.arc.ox.ac.uk\n",
    "## support@arc.ox.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bad news and good news\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### Bad news\n",
    "* serial software does not run faster on newer hardware than on old\n",
    "* *parallel processing* has been the key to increased performance for the past decade\n",
    "<br><br>\n",
    "\n",
    "### Good news\n",
    "* there are relatively easy ways for parallel programming in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Parallel processing: motivation\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "###  Traditional motivation\n",
    "* make large simulations *possible* -- spread the memory load by running on several hosts\n",
    "* make large simulations *faster* -- spread the computational load (more CPUs doing a fixed amount of work)\n",
    "<br><br>\n",
    "\n",
    "### Multicore design\n",
    "* all systems (servers, desktops, smart phones) are *parallel*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Parallel processing: motivation (cont'd)\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "###  Moore's law:\n",
    "* the number of transistors in CPU design doubles every 2 years\n",
    "<br><br>\n",
    "\n",
    "### Speed for free: CPU design until cca. 2005\n",
    "* the doubling in the number of transistors has correlated well with a doubling in speed owing to\n",
    "  * clock speed increase and\n",
    "  * instruction level parallelism\n",
    "* for over 40 years, Moore's law has practically meant a doubling in CPU performance every 2 years\n",
    "<br><br>\n",
    "\n",
    "### Cores for free: CPU design after 2005\n",
    "* reached a power wall (heat generated by the chip cannot be dissipated fast enough)\n",
    "* froze clock speed at just under 3GHz\n",
    "* has gone *multicore*\n",
    "\n",
    "<img src=\"./images/cpu_clockspeed.jpg\"; style=\"float: center; width: 60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel processing: multicore systems\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### CPUs are not getting faster anymore, they are just getting more cores\n",
    "<br><br>\n",
    "\n",
    "### The promise: all cores can be harnessed for processing\n",
    "* result: overall performance is proportional to the number of cores\n",
    "<br><br>\n",
    "\n",
    "### The reality: performance needs careful programming\n",
    "* cores are *fast*, access to data (memory, network, files) is *slow*\n",
    "* programming needs some care to extract performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory access\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### Memory access is a \"traditional\" limitation\n",
    "* the bus interconnect between CPU and main memory is a bandwidth limitation\n",
    "  * *e.g.* a 4-core Intel Core i7 + DDR3 RAM\n",
    "    * memory bandwidth 25.6GB/s max (2 memory channels)\n",
    "    * peak theoretical speed (single precision): 100 -- 190 GFlops (depending on frequency)\n",
    "* the main memory has large latency\n",
    "<br><br>\n",
    "\n",
    "### CPUs are starved of data\n",
    "* the bus is slower to \"feed\" data than the rate at which the CPU can \"consume\"\n",
    "* theoretical peak performance is useless\n",
    "  * the HPL (Linpack) benchmark achieves 90% of peak because it is designed to\n",
    "  * real world applications do not come this close\n",
    "* caching alleviates the problem (up to a point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory access (cont'd)\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### Cache = *fast* (but *small*) memory that sits between CPU and main memory\n",
    "* all program data is stored in main memory (*slow* but *large*)\n",
    "* often used data is stored temporarily in cache (*fast* but *small*)\n",
    "<br><br>\n",
    "\n",
    "### Cached reads (easiest to understand)\n",
    "* a CPU memory read from main memory fetches a whole *cache line*\n",
    "  * data requested by current instruction plus\n",
    "  * data *likely* to be requested by next instructions\n",
    "* assumptions about data already requested\n",
    "  * is physically stored near data to be requested in future (*spatial locality*)\n",
    "  * is soon going to be requested again (*temporal locality*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory access (cont'd)\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### Cached reads (1 cache level)\n",
    "* when data is requested (read) from memory, the cache line containing the data (plus adjacent data) is transfered to cache\n",
    "* subsequent reads will look in cache first\n",
    "  * *cache hit* -- data already in the cache line stored in cache (fast memory access)\n",
    "  * *cache miss* -- data not cached, send request for another cache line from main memory (slow memory access)\n",
    "* the size of cache line of x86 architecture is 64 bytes (16 integers/floats or 8 doubles)\n",
    "  * *e.g.*: if `x[20]` is requested (`float *x`), a whole cachel line containing `x[16]` through to to `x[31]` is transferred\n",
    "<br><br>\n",
    "\n",
    "### Cache writes are similar\n",
    "* https://en.wikipedia.org/wiki/Cache_(computing) is instructive<br><br>\n",
    "<br><br>\n",
    "\n",
    "### Caching is transparent to applications\n",
    "* applications do not \"see\" the cache levels (only memory access)\n",
    "* but they \"feel\" it (good or bad performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory access (cont'd)\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### NUMA architecture further complicates things\n",
    "<br><br>\n",
    "\n",
    "### **N**on-**U**niform **M**emory **A**ccess architecture\n",
    "* modern systems have multiple multicore CPUs\n",
    "* each CPU is attached (via a bus) to a *part* ot the global memory\n",
    "* CPU plus its part of memory = NUMA node\n",
    "* *all* memory is seen from *anywhere* but \"far\" memory is accessed more slowly than \"near\" memory\n",
    "<br><br>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Uniform Access</th>\n",
    "    <th>NUMA (modern software)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"./images/numa-node-1.png\"; style=\"float: center; width: 100%\"></th>\n",
    "    <th><img src=\"./images/numa-node-4.png\"; style=\"float: center; width: 100%\"></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory access (cont'd)\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### The lesson\n",
    "* computation is cheap (almost free)\n",
    "* memory access is costly\n",
    "<br><br>\n",
    "\n",
    "### What we can do\n",
    "* memory access cannot be avoided but\n",
    "* application performance is optimal through\n",
    "  * good programming\n",
    "  * careful run configuration\n",
    "<br><br>\n",
    "\n",
    "### Programming: respect data locality\n",
    "* single core: *minimise cache misses*\n",
    "  * *e.g* traverse a matrix in the right way (*e.g.* row-wise in Python and C)\n",
    "* multiple cores: *minimise remote data access*\n",
    "  * *e.g.* initialise data using the same multithreaded scheduling used to process data (*i.e.* respect first touch policy in memory initialisation)\n",
    "<br><br>\n",
    "\n",
    "### Running\n",
    "* avoiding process and thread migration\n",
    "  * *e.g.* process and/or thread pinning\n",
    "<br><br>\n",
    "\n",
    "> *Note*: network is another parallel performance factor but only affects distributed computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hardware options for parallel programming\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### Specialised harware: clusters\n",
    "* collection of individual hosts that work together in a tightly connected fashion\n",
    "  * hosts are connected via a fast network, exploitable via appropriate programming\n",
    "  * the entire cluster (or a portion of it) works as a single systems\n",
    "* *pros*\n",
    "  * the key to large simulations\n",
    "  * parallel solutions can be scaled to large number of CPUs\n",
    "* *cons*\n",
    "  * more difficult to program than a single machine (or host)\n",
    "  * achieving good scaling is not easy\n",
    "  * Python does not fit well and solutions (*e.g.* `mpi4py`) are acceptable at best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hardware options for parallel programming (cont'd)\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### Commodity hardware: laptops, workstations, servers, ...\n",
    "* multicore CPU (or CPUs)\n",
    "* shared common memory\n",
    "* *pros*\n",
    "  * fits well with general Python programmability\n",
    "  * relatively easy programming solutions\n",
    "* *cons*\n",
    "  * memory and parallel processing are both limited by what is available on a single system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Programming models\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### In order to run large simulations\n",
    "* exploit the multiple cores available in a single host and/or\n",
    "* expand parallel processing beyond a single host and run in a distributed fashion on multiple hosts\n",
    "<br><br>\n",
    "\n",
    "### Two memory models for parallel programming\n",
    "* *shared memory*\n",
    "* *distributed memory*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Programming models: distributed memory\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### The programming model\n",
    "* multi-core system, each core has its own private memory\n",
    "* local core memory is invisible to all other processors\n",
    "* agent of parallelism: the *process* (the program = a collection of processes)\n",
    "* exchanging information between processes requires *explicit* message passing\n",
    "* the dominant programming standard: *MPI*\n",
    "<br><br>\n",
    "\n",
    "### Hardware\n",
    "* clusters\n",
    "* single host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Programming models: distributed memory (cont'd)\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "<img src=\"./images/prog-model-distrib.png\"; style=\"float: center; width: 80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Programming models: shared memory\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### Shared Memory Programming Model\n",
    "* multi-core system\n",
    "* each core has access to a shared memory space\n",
    "* agent of parallelism: the *thread* (program = collection of threads)\n",
    "* threads exchange information *implicitly* by reading/writing shared variables\n",
    "* the dominant programming standard: *OpenMP*\n",
    "<br><br>\n",
    "\n",
    "### Hardware\n",
    "* single host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Programming models: shared memory (cont'd)\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "<img src=\"./images/prog-model-shared.png\"; style=\"float: center; width: 80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python solutions\n",
    "<hr style=\"border: solid 4px green\">\n",
    "\n",
    "### Four solutions\n",
    "* single system processing\n",
    "  * C/Fortran extensions using OpenMP (data parallelism)\n",
    "  * `numba` (data parallelism)\n",
    "  * `cython` (data parallelism)\n",
    "  * `multiprocessing` (task parallelism)\n",
    "* distributed processing\n",
    "  * `mpi4py`\n",
    "<br><br>\n",
    "\n",
    "### Alternatives\n",
    "* shared memory processing\n",
    "  * PyCuda, PyOpenCL\n",
    "  * OpenACC\n",
    "  * OpenMP 4 (capability to offload to GPU)\n",
    "* distributed memory processing\n",
    "  * ParallelIPython\n",
    "  * IPython cluster\n",
    "  * Apache Spark\n",
    "  * pypar, pyMPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nomenclature\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### Distinction between process and processor\n",
    "* *processor* = a physical piece of hardware\n",
    "* *process* = an instance of a computer program (software)\n",
    "  * essentially it has two components: instructions to execute and associated data\n",
    "  * in parallel programming, we often have multiple instances (processes) of the same program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nomenclature (cont'd)\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### Distinction between process and thread\n",
    "* *thread* = the smallest unit of processing the opetating system can handle (*i.e.* to which it allocates processor time)\n",
    "* a process always consists of one or more *threads* of execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nomenclature (cont'd)\n",
    "<hr style=\"border: solid 4px green; \">\n",
    "\n",
    "### Distinction between process and thread\n",
    "* *implicit sharing*\n",
    "  * threads share the memory and state of the parent process\n",
    "  * processes share nothing\n",
    "* *explicit communication*\n",
    "  * processes use inter-process communication to share data\n",
    "  * threads share data implicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<img src=\"../../images/reusematerial.png\"; style=\"float: center; width: 90\"; >\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
